8.4 The String Type
===================

The String type is the set of all finite ordered sequences of zero or more 16-bit unsigned integer
values (“elements”). The String type is generally used to represent textual data in a running
ECMAScript program, in which case each element in the String is treated as a code unit value (see
Clause 6). Each element is regarded as occupying a position within the sequence. These positions are
indexed with nonnegative integers. The first element (if any) is at position 0, the next element (if
any) at position 1, and so on. The length of a String is the number of elements (i.e., 16-bit
values) within it. The empty String has length zero and therefore contains no elements.

When a String contains actual textual data, each element is considered to be a single UTF-16 code
unit. Whether or not this is the actual storage format of a String, the characters within a String
are numbered by their initial code unit element position as though they were represented using
UTF-16. All operations on Strings (except as otherwise stated) treat them as sequences of
undifferentiated 16-bit unsigned integers; they do not ensure the resulting String is in normalised
form, nor do they ensure language-sensitive results.

NOTE The rationale behind this design was to keep the implementation of Strings as simple and
high-performing as possible. The intent is that textual data coming into the execution environment
from outside (e.g., user input, text read from a file or received over the network, etc.) be
converted to Unicode Normalised Form C before the running program sees it. Usually this would occur
at the same time incoming text is converted from its original character encoding to Unicode (and
would impose no additional overhead). Since it is recommended that ECMAScript source code be in
Normalised Form C, string literals are guaranteed to be normalised (if source text is guaranteed to
be normalised), as long as they do not contain any Unicode escape sequences.
